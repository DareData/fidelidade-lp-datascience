# Improving a model
In this section, we will delve a little bit deeper in ML specialization and we will explore some extra techniques on how to improve an existing model.

But first of all, let us recall that sticking to principles is the most important part.

## What have we learned so far?

- Feature selection and Data sufficiency, that you can recall [here](../01_ds_basics/SLU12%20-%20Data%20Sufficiency%20and%20Selection/Learning%20notebook.ipynb);
- Feature creation is also a really important one, recall [this module](../03_data_wrangling/BLU02%20-%20Advanced%20Wrangling/);
- Add more data? Interpreting [learning curves](../01_ds_basics/SLU12%20-%20Data%20Sufficiency%20and%20Selection/Learning%20notebook.ipynb);
- Hyperparameter tuning, see [this section](../01_ds_basics/SLU13%20-%20Hyperparameter%20Tuning/), combined with experiment tracking, recall [this learning unit](../02_mlops/01_mlflow/);
- Ensamble methods, that you can check [in here](../04_advanced_classification/06_ensamble_methods/Learning%20Notebook.ipynb). In particular, you can read about _bagging_, _boosting_ and _stacking_.


## What other things will you learn in here?

- From baseline to production
- Your biggest friend: [statistical tests](../05_improving_a_model/00_statistics/Learning%20Notebook.ipynb);
- [Outlier detection techniques](../05_improving_a_model/01_outlier_detection/Learning%20Notebook.ipynb);
- SHapley Additive exPlanations [(SHAP) values](../05_improving_a_model/02_SHAP/README.md);
- Extra [feature selection techniques](../05_improving_a_model/03_feature_selection/Learning%20Notebook.ipynb);
- [Dimensionality reduction](../05_improving_a_model/04_dimensionality_reduction/Learning%20Notebook.ipynb);
- Extra techniques to deal with [imbalanced datasets](../05_improving_a_model/05_imbalanced_datasets/Learning%20Notebook.ipynb);
- [Error Analysis](../05_improving_a_model/06_error_analysis/README.md) and how to interpret machine learning outcomes and failures;
- Leveraging [AutoML](../05_improving_a_model/07_autoML/README.md).